---

title: 'Deep Learning - 딥러닝 이론 정리 4 규제, 일반화 관련 이론 정리(Weight Decay, Dropout, Weigth Constraint, EarlyStopping)'

categories: ['Data Science', 'Deep Learning']

tags: 
- 딥러닝

use_math: true

toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"

---


### 정규화 / 표준화 / 일반화(균일화)

데이터 사이언스를 공부하다보면 이러한 표현들이 많은데, 이를 짚어보도록 한다.

#### 정규화(Normalization): 
데이터요소들을 0~1 사이의 값으로 변환한다. 보통 데이터중 최고값을 나눠줌으로써 진행.  
scale 자체가 높은 특성에 많은 중요도가 올라가는 경우를 방지할 수 있다.  

#### 표준화(Standardization): 
데이터요소들을 평균이 0이고 분산이 1인 분포로 변형하는 것. 표준정규분포로써 68%, 96%, 99%까지의 구간으로 나눠지며 이외의 구간에 있는 경우 outlier로 판별 가능  

#### 일반화, 균일화(Regularization):
이 역시도 정규화라는 표현과 혼용되어 표현되기도 하는데, 과적합을 방지하기 위한 '규제'를 넣는 점에서 일반화라는 표현으로도 대체할 수 있다. Ridge회귀에서도 사용되면 $\lambda$라는 표기로 사용된다.  


## 딥러닝에서의 규제 (Penalty)

딥러닝을 진행하는데에 있어서도 과적합은 존재하며, 이를 방지하기 위한 장치들이 잘 마련되어 있다.  

### Weight Decay 
학습 데이터셋에 대한 Global minima에 의존하게 되면 과적합이 될 수 있다. 그런 부분을 해소하고자 규제를 가하는 것으로 L1, L2 Norm Regularization이 존재한다.  

규제 penalty $\lambda$를 담고 있는 가중치 항을 손실함수(LOSS)에 더해줌으로써 Penalty가 커질수록 그로 인한 손실함수가 커지는 막기위해 가중치는 상응하는 만큼 줄어들도록 하는 방법이다.  

손실함수가 MSE일 때 식을 기재하면 다음과 같이 정리할 수 있다.  
L1 norm regularization=$\frac{1}{2}\sum_i(예측값_i-실제값_i)^2+\lambda|\theta w|$  
L2 norm regularization=$\frac{1}{2}\sum_i(예측값_i-실제값_i)^2+\lambda||\theta w||_2  

### Weight Constraint
특정 Weight를 범위로 선정해 넘어가지 않도록 좀 더 강제적인 규제이다.  
어떠한 연산이 진행되었거나 활성함수를 적용한 결과가 [2,4,6]인데 Weight Constraint를 5로 할 경우, [2,4,5]가 반환이 된다. 많이 사용되는 편은 아니다.  

### Dropout
노드 중 일부를 off하는 방법으로, 매 iteration마다 random하게 선정하여 노드를 끄고 학습을 해준다.   
기존에 100% 노드들이학습하는 양을 더 적은 노드들이 학습하면 노드 한개당 학습이 더 많아지기 때문에 노드들이 더 효율적으로 많이 학습할 수 있다.  

keras 라이브러리 사용시, Dense layer마다 사용할 수 있으며 꺼줄 노드의 확률만큼을 입력한다.  
해당 Dense layer에서 20%의 노드를 끄고 학습하고자 할때, Dropout(0.2)와 같이 표기해서 사용한다.  
### EarlyStopping
학습데이터를 학습하면서 검증 데이터셋과의 비교를 통해 모델 성능을 파악해, 학습데이터 셋에 대한 loss와 검증데이터 셋에 대한 model의 loss를 파악해, 그 각각의 loss의 분산(variance)가 커지는 순간 학습을 종료한다.  

이는 학습데이터 셋을 학습할 때부터 검증데이터 셋과 함께 model의 성능을 check해줌으로써 그 괴리가 커지는 순간 과적합으로 가는것임을 파악하고 학습을 종료하도록 설정하는 방법이다.  

keras에서의 사용은 callback 함수를 사용해서 설정한다.  
