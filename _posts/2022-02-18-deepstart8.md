---

title: 'Deep Learning - NLP 자연어처리 정리 2(RNN, LSTM, 순환신경망, Attention)'

categories: ['Data Science', 'Deep Learning']

tags: 
- 딥러닝

use_math: true

toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"

---

## RNN(Recurrent Neural Network)-순환신경망

RNN은 순환 신경망구조로써 기존까지 알고 있던 단순 퍼셉트론과는 약간 다른 개념을 지니고 있다.

가장 큰 특징으로는 시퀀스 개념을 가지고 있어, 이전 정보를 토대로 다음 정보를 예측하는 방식이다. 

### RNN의 구조와 예측 방식


![](https://images.velog.io/images/dlskawns/post/df9cb9fb-97bc-4dad-b0d5-de20f7232021/image.png)

$X$ = 인풋값 X  
$W_x$ = 인풋 $X$에 대한 가중치, X와 곱해준다.  
$W_h$ = 이전 레이어의 출력값 $h_{t-1}$에 대한 가중치. $h_{t-1}$과 곱해준다.   
$b$ = 편향(bias)값  
$Y_t=tanh(W_h*h_{t-1}+X_t*W_x)+b$  
$t$ =  이번 타임스텝  



위 이미지로 순환신경망의 구조를 살펴보면, 왼쪽이 신경망의 원 상태이며, 신경망 모델이 단어를 예측하는 과정을 펴내면 오른쪽과 같이 펴낼 수 있다.  

순환신경망은 설명에서 언급했듯이 시퀀스가 존재한다. 타임 스텝으로 이전 정보를 가진 채로(이미지 상에서의 +$W_hh_1$ 등)다음 타임 스텝으로 이동해 결과를 가져온다.  


이미지의 오른쪽처럼 펼쳐냈을 때의 순환신경망의 예측 과정을 설명하면 다음과 같다.  
* 입력값 $x_1$을 기입하고, 그에 대한 가중치 $W_{x1}$를 곱한 뒤 편향값 $b$를 더해준다. ($h_1$)  
* 그렇게 구해진 $h_1$값을 tanh하여 나온 값을 은닝층을 거쳐 최종 Softmax 함수를 취해 확률이 가장 높은 값을 예측한다. ($Y_1$)  
* $h_1$을 tanh한 값은 다음 타임스텝에도 사용이 되는데, 이를 위한 별도의 가중치 $W_{h1}$과 곱해준다.  
* 그 후 두번째 스텝의 입력값 $x_2$와 그에 대한 가중치 $W_{x2}$의 곱에 편향 $b$와 함께 더한다.   
($h_2 = h_1*W_{h1} + x_2*W_{x2}$)  
* $h_2$에 대한 tanh를 구한 뒤, 은닉층을 거쳐 Softmax 함수를 취해 확률이 가장 높은 값을통해 $Y_2$를 예측한다.  

위와 같은 방식으로 계속해서 이전 데이터에 대한 연산과 가중치의 곱을 더해주면서 예측하는 방식이 RNN 구조의 순환신경망의 예측 방법이다.  

### RNN의 특징(장단점)
#### 장점:
RNN의 장점이라고 할 것이 사실 지금은 개선된 추가 모델이 굉장히 많지만 굳이 언급하자면, 시퀀스의 존재로 인해 이전 단어에 대한 정보를 갖게 된다. 좀 더 문맥에 대한 정보를 가져갈 수 있는 장점을 갖고 있다.  

#### 단점:
**ⓐ 기울기 소실/폭발:**  

![](https://images.velog.io/images/dlskawns/post/e183dc4e-fff2-4731-bd0f-d3776f5ac16e/image.png)

위 그림은 순환신경망에 대한 역전파 학습을 펼쳐낸 것이다. E4에서 x1 가중치에 대한 학습을 위해 역전파를 할 경우 $\frac{\delta E4}{\delta x_1}=\frac{\delta E_4}{\delta h_4}*\frac{\delta h_4}{\delta h_3}*\frac{\delta h_3}{\delta h_2}*\frac{\delta h_2}{\delta h_1}*\frac{\delta h_1}{\delta x_1}$의 과정을 거치게 된다. 이 과정을 보면 h로 이뤄진 모든 타임스텝을 거슬러 올라가는 것을 알 수 있는데, 문장이 100개면 100번 이 미분값을 곱해주게 된다.   

RNN은 그 과정에 tanh의 활성함수가 존재하기 때문에, -1~1 사이의 값을 만드는 역할을 하고, 매우 작은 수로 계속 곱해질 경우는 0에 가까워질 수 있으며(Gradiend Vanishing), 또는 1이상이되면서 말도 안되게 증폭되는(Gradient Exploding) 현상이 발생할 수 있다.  

**ⓑ 병렬처리 불가능:**  
시퀀스를 갖는 장점이 있지만, 단점으로는 병렬처리가 안되어 GPU의 이점을 살리지 못하고, 작업을 모두 순서대로 해야하기 때문에 시간이 오래걸린다. 효율면에서도 떨어진다고 볼 수 있다.  

### LSTM(Long Short Term Memory) 장단기 기억망:

RNN을 베이스로 **기울기 소실/폭발 관련 단점을 해결**한 순환 신경망의 응용으로 현재도 많이 사용하는 모델이라고 한다.  

![](https://images.velog.io/images/dlskawns/post/0ccfd39c-71e2-4dae-80b9-6055b881b490/image.png)  

위 이미지는 RNN의 변형 LSTM의 구조이다. 다소 복잡해 보이지만 몇 가지 특징만 알면 잘 파악할 수 있다.  

ⓐ Memory를 담당하는 Cell state가 추가되었다:   
* 위 이미지의 C1, C2는 Cell state로써 기억을 담당하는 state이다.  
* f2와 i2를 더해주기만 하고 따로 활성함수를 적용시키지 않으므로 이로인한 역전파 시 기울기 소실, 폭발 문제를 해결했다.  

ⓑ Gate의 추가:  
* Forget, Input, Output 관련 Gate장치를 넣어 이전 정보를 많이 상기시킬 것인지, 혹은 새로운 입력값과 연산에 대한 가중치를 얼마나 높일 지 파악하도록 했다.  
* Forget: 이전 정보에 대한 가중치를 계산  
* Input: 새로운 정보에 대한 가중치 계산  
* Output: 이전 정보와 새로운 정보의 연산 값에 대한 가중치 계산  

#### 기계번역에서의 ENCODER와 DECODER의 역할:

![](https://images.velog.io/images/dlskawns/post/711ac802-3b85-4d8c-8bd6-d5627ea7115c/image.png)  

**ENCODER:**
각 타임스텝 별로 각 단어벡터를 입력받아 임베딩 및 순환 연산을 진행한 뒤 각 단어 벡터 정보를 담은 Hidden state vector를 Context vector로 생성해 디코더로 넘겨준다.  
입력신호는 <$sos$>부터 시작하여 각 단어에 대한 숫자 임베딩을 진행하여 사전 처럼 맵을 만든다. 이를 디코더에 최종 Context vector로 넘겨주면 이를 참고해 각 단어에 대한 의미와 문맥을 파악한다.  
  
**DECODER:**
번역을 원하는 단어의 정보와 문장에서의 순서가 어떻게 될지 문맥정보를 전달받은 Context vector로부터 파악하고, 단어의 순서를 정하고, 원하는 문장을 번역한 예측 결과를 반환한다.  

#### LSTM의 개선점:
* RNN의 기울기 관련 문제를 개선하긴 했지만, 단어가 길수록 맨 앞쪽의 단어는 정보가 전달이 안될 수 있다.  
* 또한 여전히 seq-to-seq 모델은 단어를 순서대로 학습해야하기 때문에 병렬화는 불가능하다.  

 
## Attention:
위의 RNN기반(LSTM, GRU 등) 모델의 문제점이었던 장기의존성 문제를 해결할 수 있는 모델이다.  

![](https://images.velog.io/images/dlskawns/post/f4a9134e-55b4-46c1-8a72-4933f0055eeb/image.png)

**기존 문제의 개선사항 1: 타입스텝 별로 Hidden state vector를 생성해 디코더에 전달**
  
맨 마지막 타임 스텝의 Hidden state vector를 생성하여 해당 벡터의 길이가 고정되었고, 이로인해 디코딩 과정에서 고정된 사이즈 이상의 단어나 문장은 진행이 안되는 단점을 가졌다.  
Attention을 이용하면 타임 스텝마다 Hidden state vector를 생성해 고정된 사이즈로 인한 문제를 해결할 수 있다.  
  
**기존 문제의 개선사항 2: 장기 의존성 문제를 해결할 수 있음**

문장이 길어지면 앞쪽에 있는 단어 정보가 점차 사라지던 문제를 각각의 hidden state vector에 대한 context vector를 생성하고, 이에 따른 디코더에서의 단어에 대한 의미 파악에서 보다 포커스를 둘 수 있는 인코더의 Context vector를 찾을 수 있도록 했다.  
  
### Attention의 원리

![](https://images.velog.io/images/dlskawns/post/e1d46683-a30d-412d-b7b6-538e77c8ee57/image.png)

* 위 이미지와 같이 Encoder의 타임 스탭별 Hidden state vector를 생성한 뒤, Decoder에 넘겨준다.  
* Decoder에서도 타임 스탭별로 Hidden state vector가 생성되는데, Encoder에서 생성된 모든 Hidden state vector를 각각 내적해 값을 얻는다.  
* 이후 해당 값들을 Softmax 함수를 적용해 확률값을 얻는다.  
해당 Softmax 값이 가장 큰 지점의 단어가 현재 Decoder의 타임 스탭에서 예측하고자 하는 단어와 가장 유사할 확률이 높은 것이라고 볼 수 있다.  
* 해당 확률값에 각각의 타임스탭별 Hidden state vector를 다시 곱해준다.  
* 이후 이를 모두 더해준 값이 해당 Decoder의 예측값을 위한 Context Vector가 된다.  

### Transformer  
Attention is all you need라는 제목의 논문에서 소개된 혁명적인 모델링 방법이다. RNN, LSTM의 모델은 시퀀스라는 강점을 가지고 있지만, 여전히 병렬처리 불가능이라는 단점을 갖고있다.   

속도, 효율 면에서도 개선된 모델을 찾기위해 RNN의 순차적인 계산을 행렬곱으로 한번에 진행할 수 있는 방법이다.  

### Transformer의 원리
Attention의 Decoder > Encoder로의 문맥확인의 주요 컨셉을 그대로 사용하되, RNN이 아닌 병렬처리로 진행한다. 순서에 대한 정보는 positional encoding을 통해 각 단어에 대한 위치정보를 더해준다.  

또한, Encoder 내에서의 Attention 연산과 Decoder 내에서의 Attention 연산을 진행하는 것은 그대로 가져가되, Encoder 블록과 Decoder 블록을 겹겹이 쌓아 병렬처리를 진행할 수 있도록 했다.  
