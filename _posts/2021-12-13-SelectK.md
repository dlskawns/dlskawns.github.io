---

title: 'Machine learning -  SelectKBest 정리, 특성공학, 최적의 특성 수를 구하기'

categories: ['Data Science', 'Machine Learning']

tags: 
- 머신러닝, 특성공학

use_math: true

toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"

---


## 특성공학
작성할 모델의 타겟에 대한 예측 성능을 끌어올리기 위해 하는 것으로 특성의 **재조합, 특성 선택, 특성 제거** 가 있다. 


### 특성의 재조합: 
주어진 특성 내에서 도메인 지식 또는 상식을 바탕으로 예측에 도움이 될 만한 새로운 특성을 만들어낸다. 

### 특성의 제거:
기계학습 모델의 특성 은 우리 모델을 헷갈리에 하는 독이 되는 녀석도 있다. 이러한 특성을 과감히 쳐내고, 필요한 특성만을 선택하는 것도 중요하다. 

### 특성의 선택:
특성공학으로는 특성을 재조합하는 방법도 있지만, 선택 그 자체도 좋은 특성공학 방법 중 하나가 될 수 있다.
주어진 데이터셋에서 특성들을 직접 시각화를 해서 보고, 상관계수를 구하는 등 다양한 EDA를 통해 추려낼 수도 있겠다. 

## SelectKBest

내가 내린 결론이 전부가 아닐 수 있는 상황을 고려해 최적의 특성을 고르고, 어떤 특성들이 있는지 파악할 수 있다.
Sikit-learn의 SelectKBest를 통해 진행할 수 있다.

### SelectKBest 작성

```python
from sklearn.model_selection import train_test_split

# 본래 타이타닉 데이터는 타깃이 alive이므로 target을 alive로 설정해준다.
target = 'alive'
features = df.drop(columns = target).columns

df_train, df_test = train_test_split(df, test_size = 0.2, random_state = 2)

X_train = df_train[features]
y_train = df_train[target]
X_test = df_test[features]
y_test = df_test[target]
```

```python
from category_encoders import OneHotEncoder

enc = OneHotEncoder(use_cat_names = True)
X_train_enc = enc.fit_transform(X_train, y_train)
X_test_enc = enc.transform(X_test)
# y_test는 null값이 있으므로 yes는 1, no는 0으로 치환한다.
y_test = [1 if i == 'yes' else 0 for i in df_test[target]]
X_train_enc
```

![](https://images.velog.io/images/dlskawns/post/75429a8a-1b84-48cf-bbf2-de13b5c0bb31/image.png)

위에서 진행한 타이타닉 데이터셋을 사용하여 train, test 셋을 나눈 뒤 categorical feature를 OnehotEncoding한다. 한 화면에 담지 못하게 column이 많아져서 보니 32개로 늘어났다. 늘어난 feature를 줄여본다.

```python
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_absolute_error, r2_score

# Null값의 처리를 위해 SimpleImputer 처리를 먼저 한다.

sim = SimpleImputer()
X_train_sim = sim.fit_transform(X_train_enc, y_train)
X_test_sim = sim.transform(X_test_enc)
X_train_sim


# 특성 선택을 위한 SelectKBest를 진행 - K는 특성 수

selector = SelectKBest(score_func = chi2, k = 10)
X_train_selected = selector.fit_transform(X_train_sim, y_train)
X_test_selected = selector.transform(X_test_sim)

# 선택된 column의 이름을 확인하는 작업

# 컬럼 수가 encoding으로 달라졌으므로 X_train_enc를 이용

all_names = X_train_enc.columns
selected_mask = selector.get_support()
selected_name = all_names[selected_mask]
selected_name
```
```
[output]

Index(['survived', 'pclass', 'sex_male', 'sex_female', 'fare', 'class_First',
       'class_Third', 'who_man', 'who_woman', 'adult_male'],
      dtype='object')
   
```
feature를 10개로 선택해서 최적의 feature만 뽑아서 나열했다. 파라미터는 아래와 같다.
* score_func: feature 선정시 어떤 기준으로 선정할지 방법을 정하는 것. 
	-- f_regressor: 일반적으로 회귀문제에서 사용한다.
    -- chi2: 일반적으로 분류문제에서 사용한다.
* k: 원하는 열의 개수를 지정한다. 
